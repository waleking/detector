{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process 20newsgroup dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download [20newsgroup dataset](http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz) and store it at /Users/huangwaleking/git/detector/20news-18828。([http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we read it by nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "newsgroups = \\\n",
    "  nltk.corpus.PlaintextCorpusReader('/Users/huangwaleking/git/detector/20news-18828', '.*/[0-9]+', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在20newsgroups中有18828个文件\n",
      "前20个文件的id是：\n",
      " ['alt.atheism/49960', 'alt.atheism/51060', 'alt.atheism/51119', 'alt.atheism/51120', 'alt.atheism/51121', 'alt.atheism/51122', 'alt.atheism/51123', 'alt.atheism/51124', 'alt.atheism/51125', 'alt.atheism/51126', 'alt.atheism/51127', 'alt.atheism/51128', 'alt.atheism/51130', 'alt.atheism/51131', 'alt.atheism/51132', 'alt.atheism/51133', 'alt.atheism/51134', 'alt.atheism/51135', 'alt.atheism/51136', 'alt.atheism/51139']\n",
      "第一个文件的内容是[u'From', u':', u'mathew', u'<', u'mathew', u'@', u'mantis', u'.', u'co', u'.', u'uk', u'>', u'Subject', u':', u'Alt', u'.', u'Atheism', u'FAQ', u':', u'Atheist', u'Resources', u'Archive', u'-', u'name', u':', u'atheism', u'/', u'resources', u'Alt', u'-', u'atheism', u'-', u'archive', u'-', u'name', u':', u'resources', u'Last', u'-', u'modified', u':', u'11', u'December', u'1992', u'Version', u':', u'1', u'.', u'0', u'Atheist', u'Resources', u'Addresses', u'of', u'Atheist', u'Organizations', u'USA', u'FREEDOM', u'FROM', u'RELIGION', u'FOUNDATION', u'Darwin', u'fish', u'bumper', u'stickers', u'and', u'assorted', u'other', u'atheist', u'paraphernalia', u'are', u'available', u'from', u'the', u'Freedom', u'From', u'Religion', u'Foundation', u'in', u'the', u'US', u'.', u'Write', u'to', u':', u'FFRF', u',', u'P', u'.', u'O', u'.', u'Box', u'750', u',', u'Madison', u',', u'WI', u'53701', u'.', u'Telephone', u':', u'(', u'608', u')', u'256', u'-', u'8900', u'EVOLUTION', u'DESIGNS', u'Evolution', u'Designs', u'sell', u'the', u'\"', u'Darwin', u'fish', u'\".', u'It', u\"'\", u's', u'a', u'fish', u'symbol', u',', u'like', u'the', u'ones', u'Christians', u'stick', u'on', u'their', u'cars', u',', u'but', u'with', u'feet', u'and', u'the', u'word', u'\"', u'Darwin', u'\"', u'written', u'inside', u'.', u'The', u'deluxe', u'moulded', u'3D', u'plastic', u'fish', u'is', u'$', u'4', u'.', u'95', u'postpaid', u'in', u'the', u'US', u'.', u'Write', u'to', u':', u'Evolution', u'Designs', u',', u'7119', u'Laurel', u'Canyon', u'#', u'4', u',', u'North', u'Hollywood', u',', u'CA', u'91605', u'.', u'People', u'in', u'the', u'San', u'Francisco', u'Bay', u'area', u'can', u'get', u'Darwin', u'Fish', u'from', u'Lynn', u'Gold', u'--', u'try', u'mailing', u'<', u'figmo', u'@', u'netcom', u'.', u'com', u'>.', u'For', u'net', u'people', u'who', u'go', u'to', u'Lynn', u'directly', u',', u'the', u'price', u'is', u'$', u'4', u'.', u'95', u'per', u'fish', u'.', u'AMERICAN', u'ATHEIST', u'PRESS', u'AAP', u'publish', u'various', u'atheist', u'books', u'--', u'critiques', u'of', u'the', u'Bible', u',', u'lists', u'of', u'Biblical', u'contradictions', u',', u'and', u'so', u'on', u'.', u'One', u'such', u'book', u'is', u':', u'\"', u'The', u'Bible', u'Handbook', u'\"', u'by', u'W', u'.', u'P', u'.', u'Ball', u'and', u'G', u'.', u'W', u'.', u'Foote', u'.', u'American', u'Atheist', u'Press', u'.', u'372', u'pp', u'.', u'ISBN', u'0', u'-', u'910309', u'-', u'26', u'-', u'4', u',', u'2nd', u'edition', u',', u'1986', u'.', u'Bible', u'contradictions', u',', u'absurdities', u',', u'atrocities', u',', u'immoralities', u'...', u'contains', u'Ball', u',', u'Foote', u':', u'\"', u'The', u'Bible', u'Contradicts', u'Itself', u'\",', u'AAP', u'.', u'Based', u'on', u'the', u'King', u'James', u'version', u'of', u'the', u'Bible', u'.', u'Write', u'to', u':', u'American', u'Atheist', u'Press', u',', u'P', u'.', u'O', u'.', u'Box', u'140195', u',', u'Austin', u',', u'TX', u'78714', u'-', u'0195', u'.', u'or', u':', u'7215', u'Cameron', u'Road', u',', u'Austin', u',', u'TX', u'78752', u'-', u'2973', u'.', u'Telephone', u':', u'(', u'512', u')', u'458', u'-', u'1244', u'Fax', u':', u'(', u'512', u')', u'467', u'-', u'9525', u'PROMETHEUS', u'BOOKS', u'Sell', u'books', u'including', u'Haught', u\"'\", u's', u'\"', u'Holy', u'Horrors', u'\"', u'(', u'see', u'below', u').', u'Write', u'to', u':', u'700', u'East', u'Amherst', u'Street', u',', u'Buffalo', u',', u'New', u'York', u'14215', u'.', u'Telephone', u':', u'(', u'716', u')', u'837', u'-', u'2475', u'.', u'An', u'alternate', u'address', u'(', u'which', u'may', u'be', u'newer', u'or', u'older', u')', u'is', u':', u'Prometheus', u'Books', u',', u'59', u'Glenn', u'Drive', u',', u'Buffalo', u',', u'NY', u'14228', u'-', u'2197', u'.', u'AFRICAN', u'-', u'AMERICANS', u'FOR', u'HUMANISM', u'An', u'organization', u'promoting', u'black', u'secular', u'humanism', u'and', u'uncovering', u'the', u'history', u'of', u'black', u'freethought', u'.', u'They', u'publish', u'a', u'quarterly', u'newsletter', u',', u'AAH', u'EXAMINER', u'.', u'Write', u'to', u':', u'Norm', u'R', u'.', u'Allen', u',', u'Jr', u'.,', u'African', u'Americans', u'for', u'Humanism', u',', u'P', u'.', u'O', u'.', u'Box', u'664', u',', u'Buffalo', u',', u'NY', u'14226', u'.', u'United', u'Kingdom', u'Rationalist', u'Press', u'Association', u'National', u'Secular', u'Society', u'88', u'Islington', u'High', u'Street', u'702', u'Holloway', u'Road', u'London', u'N1', u'8EW', u'London', u'N19', u'3NL', u'071', u'226', u'7251', u'071', u'272', u'1266', u'British', u'Humanist', u'Association', u'South', u'Place', u'Ethical', u'Society', u'14', u'Lamb', u\"'\", u's', u'Conduit', u'Passage', u'Conway', u'Hall', u'London', u'WC1R', u'4RH', u'Red', u'Lion', u'Square', u'071', u'430', u'0908', u'London', u'WC1R', u'4RL', u'fax', u'071', u'430', u'1271', u'071', u'831', u'7723', u'The', u'National', u'Secular', u'Society', u'publish', u'\"', u'The', u'Freethinker', u'\",', u'a', u'monthly', u'magazine', u'founded', u'in', u'1881', u'.', u'Germany', u'IBKA', u'e', u'.', u'V', u'.', u'Internationaler', u'Bund', u'der', u'Konfessionslosen', u'und', u'Atheisten', u'Postfach', u'880', u',', u'D', u'-', u'1000', u'Berlin', u'41', u'.', u'Germany', u'.', u'IBKA', u'publish', u'a', u'journal', u':', u'MIZ', u'.', u'(', u'Materialien', u'und', u'Informationen', u'zur', u'Zeit', u'.', u'Politisches', u'Journal', u'der', u'Konfessionslosesn', u'und', u'Atheisten', u'.', u'Hrsg', u'.', u'IBKA', u'e', u'.', u'V', u'.)', u'MIZ', u'-', u'Vertrieb', u',', u'Postfach', u'880', u',', u'D', u'-', u'1000', u'Berlin', u'41', u'.', u'Germany', u'.', u'For', u'atheist', u'books', u',', u'write', u'to', u':', u'IBDK', u',', u'Internationaler', u'B', u'\"', u'ucherdienst', u'der', u'Konfessionslosen', u'Postfach', u'3005', u',', u'D', u'-', u'3000', u'Hannover', u'1', u'.', u'Germany', u'.', u'Telephone', u':', u'0511', u'/', u'211216', u'Books', u'--', u'Fiction', u'THOMAS', u'M', u'.', u'DISCH', u'\"', u'The', u'Santa', u'Claus', u'Compromise', u'\"', u'Short', u'story', u'.', u'The', u'ultimate', u'proof', u'that', u'Santa', u'exists', u'.', u'All', u'characters', u'and', u'events', u'are', u'fictitious', u'.', u'Any', u'similarity', u'to', u'living', u'or', u'dead', u'gods', u'--', u'uh', u',', u'well', u'...', u'WALTER', u'M', u'.', u'MILLER', u',', u'JR', u'\"', u'A', u'Canticle', u'for', u'Leibowitz', u'\"', u'One', u'gem', u'in', u'this', u'post', u'atomic', u'doomsday', u'novel', u'is', u'the', u'monks', u'who', u'spent', u'their', u'lives', u'copying', u'blueprints', u'from', u'\"', u'Saint', u'Leibowitz', u'\",', u'filling', u'the', u'sheets', u'of', u'paper', u'with', u'ink', u'and', u'leaving', u'white', u'lines', u'and', u'letters', u'.', u'EDGAR', u'PANGBORN', u'\"', u'Davy', u'\"', u'Post', u'atomic', u'doomsday', u'novel', u'set', u'in', u'clerical', u'states', u'.', u'The', u'church', u',', u'for', u'example', u',', u'forbids', u'that', u'anyone', u'\"', u'produce', u',', u'describe', u'or', u'use', u'any', u'substance', u'containing', u'...', u'atoms', u'\".', u'PHILIP', u'K', u'.', u'DICK', u'Philip', u'K', u'.', u'Dick', u'Dick', u'wrote', u'many', u'philosophical', u'and', u'thought', u'-', u'provoking', u'short', u'stories', u'and', u'novels', u'.', u'His', u'stories', u'are', u'bizarre', u'at', u'times', u',', u'but', u'very', u'approachable', u'.', u'He', u'wrote', u'mainly', u'SF', u',', u'but', u'he', u'wrote', u'about', u'people', u',', u'truth', u'and', u'religion', u'rather', u'than', u'technology', u'.', u'Although', u'he', u'often', u'believed', u'that', u'he', u'had', u'met', u'some', u'sort', u'of', u'God', u',', u'he', u'remained', u'sceptical', u'.', u'Amongst', u'his', u'novels', u',', u'the', u'following', u'are', u'of', u'some', u'relevance', u':', u'\"', u'Galactic', u'Pot', u'-', u'Healer', u'\"', u'A', u'fallible', u'alien', u'deity', u'summons', u'a', u'group', u'of', u'Earth', u'craftsmen', u'and', u'women', u'to', u'a', u'remote', u'planet', u'to', u'raise', u'a', u'giant', u'cathedral', u'from', u'beneath', u'the', u'oceans', u'.', u'When', u'the', u'deity', u'begins', u'to', u'demand', u'faith', u'from', u'the', u'earthers', u',', u'pot', u'-', u'healer', u'Joe', u'Fernwright', u'is', u'unable', u'to', u'comply', u'.', u'A', u'polished', u',', u'ironic', u'and', u'amusing', u'novel', u'.', u'\"', u'A', u'Maze', u'of', u'Death', u'\"', u'Noteworthy', u'for', u'its', u'description', u'of', u'a', u'technology', u'-', u'based', u'religion', u'.', u'\"', u'VALIS', u'\"', u'The', u'schizophrenic', u'hero', u'searches', u'for', u'the', u'hidden', u'mysteries', u'of', u'Gnostic', u'Christianity', u'after', u'reality', u'is', u'fired', u'into', u'his', u'brain', u'by', u'a', u'pink', u'laser', u'beam', u'of', u'unknown', u'but', u'possibly', u'divine', u'origin', u'.', u'He', u'is', u'accompanied', u'by', u'his', u'dogmatic', u'and', u'dismissively', u'atheist', u'friend', u'and', u'assorted', u'other', u'odd', u'characters', u'.', u'\"', u'The', u'Divine', u'Invasion', u'\"', u'God', u'invades', u'Earth', u'by', u'making', u'a', u'young', u'woman', u'pregnant', u'as', u'she', u'returns', u'from', u'another', u'star', u'system', u'.', u'Unfortunately', u'she', u'is', u'terminally', u'ill', u',', u'and', u'must', u'be', u'assisted', u'by', u'a', u'dead', u'man', u'whose', u'brain', u'is', u'wired', u'to', u'24', u'-', u'hour', u'easy', u'listening', u'music', u'.', u'MARGARET', u'ATWOOD', u'\"', u'The', u'Handmaid', u\"'\", u's', u'Tale', u'\"', u'A', u'story', u'based', u'on', u'the', u'premise', u'that', u'the', u'US', u'Congress', u'is', u'mysteriously', u'assassinated', u',', u'and', u'fundamentalists', u'quickly', u'take', u'charge', u'of', u'the', u'nation', u'to', u'set', u'it', u'\"', u'right', u'\"', u'again', u'.', u'The', u'book', u'is', u'the', u'diary', u'of', u'a', u'woman', u\"'\", u's', u'life', u'as', u'she', u'tries', u'to', u'live', u'under', u'the', u'new', u'Christian', u'theocracy', u'.', u'Women', u\"'\", u's', u'right', u'to', u'own', u'property', u'is', u'revoked', u',', u'and', u'their', u'bank', u'accounts', u'are', u'closed', u';', u'sinful', u'luxuries', u'are', u'outlawed', u',', u'and', u'the', u'radio', u'is', u'only', u'used', u'for', u'readings', u'from', u'the', u'Bible', u'.', u'Crimes', u'are', u'punished', u'retroactively', u':', u'doctors', u'who', u'performed', u'legal', u'abortions', u'in', u'the', u'\"', u'old', u'world', u'\"', u'are', u'hunted', u'down', u'and', u'hanged', u'.', u'Atwood', u\"'\", u's', u'writing', u'style', u'is', u'difficult', u'to', u'get', u'used', u'to', u'at', u'first', u',', u'but', u'the', u'tale', u'grows', u'more', u'and', u'more', u'chilling', u'as', u'it', u'goes', u'on', u'.', u'VARIOUS', u'AUTHORS', u'\"', u'The', u'Bible', u'\"', u'This', u'somewhat', u'dull', u'and', u'rambling', u'work', u'has', u'often', u'been', u'criticized', u'.', u'However', u',', u'it', u'is', u'probably', u'worth', u'reading', u',', u'if', u'only', u'so', u'that', u'you', u\"'\", u'll', u'know', u'what', u'all', u'the', u'fuss', u'is', u'about', u'.', u'It', u'exists', u'in', u'many', u'different', u'versions', u',', u'so', u'make', u'sure', u'you', u'get', u'the', u'one', u'true', u'version', u'.', u'Books', u'--', u'Non', u'-', u'fiction', u'PETER', u'DE', u'ROSA', u'\"', u'Vicars', u'of', u'Christ', u'\",', u'Bantam', u'Press', u',', u'1988', u'Although', u'de', u'Rosa', u'seems', u'to', u'be', u'Christian', u'or', u'even', u'Catholic', u'this', u'is', u'a', u'very', u'enlighting', u'history', u'of', u'papal', u'immoralities', u',', u'adulteries', u',', u'fallacies', u'etc', u'.', u'(', u'German', u'translation', u':', u'\"', u'Gottes', u'erste', u'Diener', u'.', u'Die', u'dunkle', u'Seite', u'des', u'Papsttums', u'\",', u'Droemer', u'-', u'Knaur', u',', u'1989', u')', u'MICHAEL', u'MARTIN', u'\"', u'Atheism', u':', u'A', u'Philosophical', u'Justification', u'\",', u'Temple', u'University', u'Press', u',', u'Philadelphia', u',', u'USA', u'.', u'A', u'detailed', u'and', u'scholarly', u'justification', u'of', u'atheism', u'.', u'Contains', u'an', u'outstanding', u'appendix', u'defining', u'terminology', u'and', u'usage', u'in', u'this', u'(', u'necessarily', u')', u'tendentious', u'area', u'.', u'Argues', u'both', u'for', u'\"', u'negative', u'atheism', u'\"', u'(', u'i', u'.', u'e', u'.', u'the', u'\"', u'non', u'-', u'belief', u'in', u'the', u'existence', u'of', u'god', u'(', u's', u')\")', u'and', u'also', u'for', u'\"', u'positive', u'atheism', u'\"', u'(\"', u'the', u'belief', u'in', u'the', u'non', u'-', u'existence', u'of', u'god', u'(', u's', u')\").', u'Includes', u'great', u'refutations', u'of', u'the', u'most', u'challenging', u'arguments', u'for', u'god', u';', u'particular', u'attention', u'is', u'paid', u'to', u'refuting', u'contempory', u'theists', u'such', u'as', u'Platinga', u'and', u'Swinburne', u'.', u'541', u'pages', u'.', u'ISBN', u'0', u'-', u'87722', u'-', u'642', u'-', u'3', u'(', u'hardcover', u';', u'paperback', u'also', u'available', u')', u'\"', u'The', u'Case', u'Against', u'Christianity', u'\",', u'Temple', u'University', u'Press', u'A', u'comprehensive', u'critique', u'of', u'Christianity', u',', u'in', u'which', u'he', u'considers', u'the', u'best', u'contemporary', u'defences', u'of', u'Christianity', u'and', u'(', u'ultimately', u')', u'demonstrates', u'that', u'they', u'are', u'unsupportable', u'and', u'/', u'or', u'incoherent', u'.', u'273', u'pages', u'.', u'ISBN', u'0', u'-', u'87722', u'-', u'767', u'-', u'5', u'JAMES', u'TURNER', u'\"', u'Without', u'God', u',', u'Without', u'Creed', u'\",', u'The', u'Johns', u'Hopkins', u'University', u'Press', u',', u'Baltimore', u',', u'MD', u',', u'USA', u'Subtitled', u'\"', u'The', u'Origins', u'of', u'Unbelief', u'in', u'America', u'\".', u'Examines', u'the', u'way', u'in', u'which', u'unbelief', u'(', u'whether', u'agnostic', u'or', u'atheistic', u')', u'became', u'a', u'mainstream', u'alternative', u'world', u'-', u'view', u'.', u'Focusses', u'on', u'the', u'period', u'1770', u'-', u'1900', u',', u'and', u'while', u'considering', u'France', u'and', u'Britain', u'the', u'emphasis', u'is', u'on', u'American', u',', u'and', u'particularly', u'New', u'England', u'developments', u'.', u'\"', u'Neither', u'a', u'religious', u'history', u'of', u'secularization', u'or', u'atheism', u',', u'Without', u'God', u',', u'Without', u'Creed', u'is', u',', u'rather', u',', u'the', u'intellectual', u'history', u'of', u'the', u'fate', u'of', u'a', u'single', u'idea', u',', u'the', u'belief', u'that', u'God', u'exists', u'.\"', u'316', u'pages', u'.', u'ISBN', u'(', u'hardcover', u')', u'0', u'-', u'8018', u'-', u'2494', u'-', u'X', u'(', u'paper', u')', u'0', u'-', u'8018', u'-', u'3407', u'-', u'4', u'GEORGE', u'SELDES', u'(', u'Editor', u')', u'\"', u'The', u'great', u'thoughts', u'\",', u'Ballantine', u'Books', u',', u'New', u'York', u',', u'USA', u'A', u'\"', u'dictionary', u'of', u'quotations', u'\"', u'of', u'a', u'different', u'kind', u',', u'concentrating', u'on', u'statements', u'and', u'writings', u'which', u',', u'explicitly', u'or', u'implicitly', u',', u'present', u'the', u'person', u\"'\", u's', u'philosophy', u'and', u'world', u'-', u'view', u'.', u'Includes', u'obscure', u'(', u'and', u'often', u'suppressed', u')', u'opinions', u'from', u'many', u'people', u'.', u'For', u'some', u'popular', u'observations', u',', u'traces', u'the', u'way', u'in', u'which', u'various', u'people', u'expressed', u'and', u'twisted', u'the', u'idea', u'over', u'the', u'centuries', u'.', u'Quite', u'a', u'number', u'of', u'the', u'quotations', u'are', u'derived', u'from', u'Cardiff', u\"'\", u's', u'\"', u'What', u'Great', u'Men', u'Think', u'of', u'Religion', u'\"', u'and', u'Noyes', u\"'\", u'\"', u'Views', u'of', u'Religion', u'\".', u'490', u'pages', u'.', u'ISBN', u'(', u'paper', u')', u'0', u'-', u'345', u'-', u'29887', u'-', u'X', u'.', u'RICHARD', u'SWINBURNE', u'\"', u'The', u'Existence', u'of', u'God', u'(', u'Revised', u'Edition', u')\",', u'Clarendon', u'Paperbacks', u',', u'Oxford', u'This', u'book', u'is', u'the', u'second', u'volume', u'in', u'a', u'trilogy', u'that', u'began', u'with', u'\"', u'The', u'Coherence', u'of', u'Theism', u'\"', u'(', u'1977', u')', u'and', u'was', u'concluded', u'with', u'\"', u'Faith', u'and', u'Reason', u'\"', u'(', u'1981', u').', u'In', u'this', u'work', u',', u'Swinburne', u'attempts', u'to', u'construct', u'a', u'series', u'of', u'inductive', u'arguments', u'for', u'the', u'existence', u'of', u'God', u'.', u'His', u'arguments', u',', u'which', u'are', u'somewhat', u'tendentious', u'and', u'rely', u'upon', u'the', u'imputation', u'of', u'late', u'20th', u'century', u'western', u'Christian', u'values', u'and', u'aesthetics', u'to', u'a', u'God', u'which', u'is', u'supposedly', u'as', u'simple', u'as', u'can', u'be', u'conceived', u',', u'were', u'decisively', u'rejected', u'in', u'Mackie', u\"'\", u's', u'\"', u'The', u'Miracle', u'of', u'Theism', u'\".', u'In', u'the', u'revised', u'edition', u'of', u'\"', u'The', u'Existence', u'of', u'God', u'\",', u'Swinburne', u'includes', u'an', u'Appendix', u'in', u'which', u'he', u'makes', u'a', u'somewhat', u'incoherent', u'attempt', u'to', u'rebut', u'Mackie', u'.', u'J', u'.', u'L', u'.', u'MACKIE', u'\"', u'The', u'Miracle', u'of', u'Theism', u'\",', u'Oxford', u'This', u'(', u'posthumous', u')', u'volume', u'contains', u'a', u'comprehensive', u'review', u'of', u'the', u'principal', u'arguments', u'for', u'and', u'against', u'the', u'existence', u'of', u'God', u'.', u'It', u'ranges', u'from', u'the', u'classical', u'philosophical', u'positions', u'of', u'Descartes', u',', u'Anselm', u',', u'Berkeley', u',', u'Hume', u'et', u'al', u',', u'through', u'the', u'moral', u'arguments', u'of', u'Newman', u',', u'Kant', u'and', u'Sidgwick', u',', u'to', u'the', u'recent', u'restatements', u'of', u'the', u'classical', u'theses', u'by', u'Plantinga', u'and', u'Swinburne', u'.', u'It', u'also', u'addresses', u'those', u'positions', u'which', u'push', u'the', u'concept', u'of', u'God', u'beyond', u'the', u'realm', u'of', u'the', u'rational', u',', u'such', u'as', u'those', u'of', u'Kierkegaard', u',', u'Kung', u'and', u'Philips', u',', u'as', u'well', u'as', u'\"', u'replacements', u'for', u'God', u'\"', u'such', u'as', u'Lelie', u\"'\", u's', u'axiarchism', u'.', u'The', u'book', u'is', u'a', u'delight', u'to', u'read', u'-', u'less', u'formalistic', u'and', u'better', u'written', u'than', u'Martin', u\"'\", u's', u'works', u',', u'and', u'refreshingly', u'direct', u'when', u'compared', u'with', u'the', u'hand', u'-', u'waving', u'of', u'Swinburne', u'.', u'JAMES', u'A', u'.', u'HAUGHT', u'\"', u'Holy', u'Horrors', u':', u'An', u'Illustrated', u'History', u'of', u'Religious', u'Murder', u'and', u'Madness', u'\",', u'Prometheus', u'Books', u'Looks', u'at', u'religious', u'persecution', u'from', u'ancient', u'times', u'to', u'the', u'present', u'day', u'--', u'and', u'not', u'only', u'by', u'Christians', u'.', u'Library', u'of', u'Congress', u'Catalog', u'Card', u'Number', u'89', u'-', u'64079', u'.', u'1990', u'.', u'NORM', u'R', u'.', u'ALLEN', u',', u'JR', u'.', u'\"', u'African', u'American', u'Humanism', u':', u'an', u'Anthology', u'\"', u'See', u'the', u'listing', u'for', u'African', u'Americans', u'for', u'Humanism', u'above', u'.', u'GORDON', u'STEIN', u'\"', u'An', u'Anthology', u'of', u'Atheism', u'and', u'Rationalism', u'\",', u'Prometheus', u'Books', u'An', u'anthology', u'covering', u'a', u'wide', u'range', u'of', u'subjects', u',', u'including', u\"'\", u'The', u'Devil', u',', u'Evil', u'and', u'Morality', u\"'\", u'and', u\"'\", u'The', u'History', u'of', u'Freethought', u\"'.\", u'Comprehensive', u'bibliography', u'.', u'EDMUND', u'D', u'.', u'COHEN', u'\"', u'The', u'Mind', u'of', u'The', u'Bible', u'-', u'Believer', u'\",', u'Prometheus', u'Books', u'A', u'study', u'of', u'why', u'people', u'become', u'Christian', u'fundamentalists', u',', u'and', u'what', u'effect', u'it', u'has', u'on', u'them', u'.', u'Net', u'Resources', u'There', u\"'\", u's', u'a', u'small', u'mail', u'-', u'based', u'archive', u'server', u'at', u'mantis', u'.', u'co', u'.', u'uk', u'which', u'carries', u'archives', u'of', u'old', u'alt', u'.', u'atheism', u'.', u'moderated', u'articles', u'and', u'assorted', u'other', u'files', u'.', u'For', u'more', u'information', u',', u'send', u'mail', u'to', u'archive', u'-', u'server', u'@', u'mantis', u'.', u'co', u'.', u'uk', u'saying', u'help', u'send', u'atheism', u'/', u'index', u'and', u'it', u'will', u'mail', u'back', u'a', u'reply', u'.', u'mathew', u'\\xff']\n",
      "newsgroups.words的类型是<class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n"
     ]
    }
   ],
   "source": [
    "ids = newsgroups.fileids()\n",
    "print(\"在20newsgroups中有%s个文件\" % len(ids))\n",
    "print(\"前20个文件的id是：\\n %s\" % ids[:20])\n",
    "print(\"第一个文件的内容是%s\" % list(newsgroups.words(fileids=ids[0])))\n",
    "print(\"newsgroups.words的类型是%s\" % type(newsgroups.words(fileids=ids[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK用于语法解析\n",
    "有关nltk的例子来自于 [PyCon2016](http://pycon.districtdatalabs.com/tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (DET an) (N airplane)))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "grammar = nltk.grammar.CFG.fromstring(\"\"\"\n",
    "S -> NP\n",
    "NP -> N N | ADJP NP | DET N\n",
    "ADJP -> ADJ NP\n",
    "DET -> 'an'\n",
    "N ->'airplane'\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.parse.ChartParser(grammar)\n",
    "p=list(parser.parse(nltk.word_tokenize(\"an airplane\")))\n",
    "for a in p:\n",
    "    a.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK用于词干提取(stemming)和lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eagle', u'fly', 'midnight']\n",
      "The stemming and lemmatization cost 0.00257587432861 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "lemmatizer= nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation=string.punctuation\n",
    "\n",
    "def normalize(text):\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        token=token.lower()\n",
    "        token=lemmatizer.lemmatize(token)\n",
    "        if token not in stopwords and token not in punctuation:\n",
    "            yield token\n",
    "\n",
    "print(list(normalize(\"The eagle flies at midnight.\")))\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"The stemming and lemmatization cost %s seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK用于命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  (PERSON Smith/NNP)\n",
      "  is/VBZ\n",
      "  from/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  of/IN\n",
      "  (GPE America/NNP)\n",
      "  and/CC\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Microsoft/NNP Research/NNP Labs/NNP))\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    nltk.ne_chunk(\n",
    "        nltk.pos_tag(\n",
    "            nltk.word_tokenize(\n",
    "                \"John Smith is from the United States of America and works at Microsoft Research Labs\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK使用Stanford NER系统进行命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PERSON] John\n",
      "[PERSON] Bengfort\n",
      "[O] is\n",
      "[O] from\n",
      "[O] the\n",
      "[LOCATION] United\n",
      "[LOCATION] States\n",
      "[LOCATION] of\n",
      "[LOCATION] America\n",
      "[O] and\n",
      "[O] works\n",
      "[O] at\n",
      "[ORGANIZATION] Microsoft\n",
      "[ORGANIZATION] Research\n",
      "[ORGANIZATION] Labs\n",
      "[O] Pandas\n",
      "[O] makes\n",
      "[O] it\n",
      "[O] super\n",
      "[O] simple\n",
      "[O] to\n",
      "[O] apply\n",
      "[O] custom\n",
      "[O] functions\n",
      "[O] over\n",
      "[O] groups\n",
      "[O] of\n",
      "[O] data.\n",
      "The NER process costs 3.45166993141 seconds\n"
     ]
    }
   ],
   "source": [
    "# change the paths below to point to wherever you unzipped the Stanford NER download file\n",
    "import os\n",
    "import nltk.tag.stanford as st\n",
    "\n",
    "stanford_root = '/Users/huangwaleking/Documents/stanford-ner-2014-01-04'\n",
    "stanford_data = os.path.join(stanford_root,'classifiers/english.all.3class.distsim.crf.ser.gz')\n",
    "stanford_jar  = os.path.join(stanford_root,'stanford-ner.jar')\n",
    "\n",
    "tagger = st.StanfordNERTagger(stanford_data, stanford_jar, 'utf-8')\n",
    "\n",
    "for tagged in tagger.tag(\n",
    "  \"John Bengfort is from the United States of America and \"\n",
    "  \"works at Microsoft Research Labs\".split()):\n",
    "    print('[' + tagged[1] + '] ' + tagged[0])\n",
    "    \n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for tagged in tagger.tag(\n",
    "  \"Pandas makes it super simple to apply custom functions over groups of data.\".split()):\n",
    "    print('[' + tagged[1] + '] ' + tagged[0])\n",
    "    \n",
    "end = time.time()\n",
    "print(\"The NER process costs %s seconds\" % (end-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK使用Distributed Representation\n",
    "文档的vector encoding表示方法有如下四种：\n",
    "(1) bag of words\n",
    "(2) one hot（在神经网络中使用较多）\n",
    "(3) TF-IDF\n",
    "(4) distributed representation (doc2vec，可以参考[论文](https://cs.stanford.edu/~quocle/paragraph_vector.pdf),或在[本地查看](http://localhost:8081/mypapers/Files/D9/D9263895-27FC-4213-9DCF-502F6EC7E7E4.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02257801, -0.02108288,  0.02712592,  0.00483102,  0.04937694,\n",
       "       -0.03264765, -0.03207875], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "documents=gensim.models.doc2vec.TaggedLineDocument(\"usedForNLTK_example.txt\")\n",
    "model = gensim.models.doc2vec.Doc2Vec(documents, size=7, min_count=0)\n",
    "model.docvecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "cropus=sklearn.datasets.fetch_20newsgroups(remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.datasets.base.Bunch"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cropus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and extracting TF-IDF features...\n",
      "done in 4.528s.\n",
      "Fitting the NMF model with n_samples=2000 and n_features=1000...\n",
      "done in 4.741s.\n",
      "Topic #0:\n",
      "god jesus believe faith christians bible christ truth people christian church rutgers does hell life evidence question atheists say true apr man jews religion did brian peace existence islam mean accept belief reason exist 1993 things religious fact word know cwru death earth world physical john come spirit muslims judge\n",
      "()\n",
      "Topic #1:\n",
      "key chip encryption clipper keys algorithm secret security government public uni chips use des communications netcom number phone information bit using message law data house device bits available standard does clinton used text door att code private technology know 80 org known ca request able administration technical end mail systems\n",
      "()\n",
      "Topic #2:\n",
      "edu cs university article nntp host posting writes cc pitt reply distribution science berkeley computer colorado caltech keith uiuc news cwru institute cmu au david stanford usa andrew thanks john utexas pittsburgh michael california cso univ looking mail 12 college cleveland dept purdue originator 13 know just like computing 15\n",
      "()\n",
      "Topic #3:\n",
      "windows card drive file dos scsi thanks use program problem mac help software using video pc memory running screen files disk monitor know bus ram version display does mit fax hi mail graphics driver drivers modem info computer like need email work machine printer ms hard host ide problems board\n",
      "()\n",
      "Topic #4:\n",
      "people don just think like car right time way make israel good say know government israeli said years ve really going want better did long gun new guns rights org law things year writes ll little power got work use let country case thing fact far arab look come turkish\n",
      "()\n",
      "Topic #5:\n",
      "com ibm writes article netcom hp sun access stratus reply usa corporation digex posting distribution austin att systems world jim nntp host bike net bob mot steve opinions pat new sgi corp dec newsreader mark good ma uunet richard east ed 14 list does support disclaimer apr internet tom version\n",
      "()\n",
      "Topic #6:\n",
      "ca game team season hockey play games canada players toronto year win roger player baseball cup university played goal fan 10 mike ice best runs league better article hit good didn time john cs division 15 night writes net like 12 san got home don think second great st vs\n",
      "()\n",
      "Topic #7:\n",
      "ohio state magnus acs edu university oil usa sale war used posting nntp 1993apr15 host drive friend car article richard netcom don cleveland modem good cars just order away self new steve andrew hands place information takes years hard john york 12 offer san james 10 number oh phone thomas\n",
      "()\n",
      "Topic #8:\n",
      "uk ac science 44 cs dept computer ve mail computing paul university problem __ gas use data writes richard server ed return article 16 ___ image course exist quality good research size 14 need know say page internet help news word posting project come line reply information doesn images address\n",
      "()\n",
      "Topic #9:\n",
      "nasa gov space moon orbit shuttle __ ___ laboratory henry center research national earth satellite project access sci net toronto digex program uiuc mark office org cso data like host food internet posting science nntp pat writes information development station world billion power cost send build mass austin distribution graphics\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 50\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "t0 = time()\n",
    "print(\"Loading dataset and extracting TF-IDF features...\")\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                             stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(dataset.data[:n_samples])\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 3.599s.\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 5.298s.\n",
      "Extracting tf features for LDA...\n",
      "done in 5.165s.\n",
      "Fitting the NMF model with tf-idf features,n_samples=2000 and n_features=1000...\n",
      "done in 4.328s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "edu com article writes don like just people university posting think host nntp know ca good cs time new distribution\n",
      "Topic #1:\n",
      "god jesus bible christians faith christian believe christ people life hell truth say church christianity religion sin heaven does existence\n",
      "Topic #2:\n",
      "pitt geb banks gordon cs cadre dsl n3jxp chastity shameful skepticism intellect surrender pittsburgh edu univ soon science computer reply\n",
      "Topic #3:\n",
      "ohio magnus state acs edu university ryan magnusug cis cleveland scharfy rscharfy nntp host drugs posting cwru article nielsen oil\n",
      "Topic #4:\n",
      "windows dos file window card files mouse ms program video use screen pc drivers problem thanks using graphics version help\n",
      "Topic #5:\n",
      "sandvik kent apple newton ksand alink com cookamunga tourist activities bureau cheers private net wrote royalroads malcolm christian ignorance jesus\n",
      "Topic #6:\n",
      "israel israeli jews arab lebanese arabs peace israelis lebanon jake adam policy jewish hernlem cpr palestinian palestinians land palestine gaza\n",
      "Topic #7:\n",
      "keith caltech livesey sgi morality solntze wpd cco jon schneider objective allan moral atheists pasadena edu political punisher gap california\n",
      "Topic #8:\n",
      "scsi drive ide controller drives bus hard mac disk isa pc hd floppy devices data dma nmsu uwo mb bit\n",
      "Topic #9:\n",
      "key clipper encryption chip escrow keys government algorithm crypto security secure privacy secret nsa enforcement des law intercon wiretap public\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 52.004s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "game team year edu ca play games turkish armenian don toronto hockey think season armenians players good like said did\n",
      "Topic #1:\n",
      "just like don time people think know work good make going way use ve new years want space really long\n",
      "Topic #2:\n",
      "edu windows drive card use thanks university does scsi uk problem know dos like mac ibm pc ac computer help\n",
      "Topic #3:\n",
      "10 00 15 20 16 1993 11 apr 25 12 14 93 30 13 17 50 18 19 000 new\n",
      "Topic #4:\n",
      "people god think don believe say does know edu jesus gun just law did said right mr christian like jews\n",
      "Topic #5:\n",
      "com writes article posting netcom host nntp window sun distribution ca edu just like don hp good reply know sgi\n",
      "Topic #6:\n",
      "145 b8f a86 0d cx 34u 0t 2di 1d9 pl ah wm ai 75u 34 g9v air 3t 04 45\n",
      "Topic #7:\n",
      "ax max g9v b8f a86 pl giz bhj 1t 1d9 75u 3t 2tm 0t wm 2di 6ei 34u 75 145\n",
      "Topic #8:\n",
      "edu writes article university posting host nntp cs cc ca distribution reply gov news state nasa com usa uiuc __\n",
      "Topic #9:\n",
      "key file information use available program data number space encryption chip public ftp mail list clipper pub code faq send\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck <L.J.Buitinck@uva.nl>\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, #max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features,\"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "exit()\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
